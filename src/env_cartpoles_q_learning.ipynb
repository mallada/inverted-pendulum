{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "from math import sin, cos, radians, log10\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import numpy as np\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "from lib.cartpolesystem import CartPoleSystem\n",
    "from lib.cartpoleenv import CartPoleEnv\n",
    "from lib.colors import Colors\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = 0.01\n",
    "g = 9.81\n",
    "\n",
    "system = CartPoleSystem(\n",
    "    (0.0, 0.5, 0.05, -0.8, 0.8, Colors.red),\n",
    "    (0.05, 0.05, 0.01, 0.5, 0.05, 24.0, Colors.black),\n",
    "    [\n",
    "        (radians(10), 0.2, 0.2, 0.005, Colors.green),\n",
    "    ],\n",
    "    g,\n",
    "    dt,\n",
    "    \"rk4\"\n",
    ")\n",
    "\n",
    "env = CartPoleEnv(system, dt, g)\n",
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low [array([-0.8]) -5 0 -5]\n",
      "High [array([0.8]) 5 6.283185307179586 5.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sacka\\AppData\\Local\\Temp\\ipykernel_12504\\771966455.py:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  obs_low = np.hstack([np.array([obs_low[0], -max_velocity]), np.tile([0, -max_velocity], system.num_poles)])\n",
      "C:\\Users\\sacka\\AppData\\Local\\Temp\\ipykernel_12504\\771966455.py:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  obs_high = np.hstack([np.array([obs_high[0], max_velocity]), np.tile([radians(360), max_velocity], system.num_poles)])\n"
     ]
    }
   ],
   "source": [
    "n_obs_bins = 11\n",
    "n_action_bins = 3\n",
    "max_velocity = 5\n",
    "\n",
    "obs_low = np.array(env.observation_space.low)\n",
    "obs_high = np.array(env.observation_space.high)\n",
    "\n",
    "obs_low = np.hstack([np.array([obs_low[0], -max_velocity]), np.tile([0, -max_velocity], system.num_poles)])\n",
    "obs_high = np.hstack([np.array([obs_high[0], max_velocity]), np.tile([radians(360), max_velocity], system.num_poles)])\n",
    "\n",
    "action_low = np.array(env.action_space.low)\n",
    "action_high = np.array(env.action_space.high)\n",
    "\n",
    "print(\"Low\", obs_low)\n",
    "print(\"High\", obs_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_state(state):\n",
    "    refined = state\n",
    "    return refined\n",
    "\n",
    "def obs_discretizer(obs) -> tuple[int,...]:\n",
    "    est = KBinsDiscretizer(n_bins=n_obs_bins, encode=\"ordinal\", strategy=\"uniform\")\n",
    "    est.fit([obs_low, obs_high])\n",
    "    return tuple(map(int, est.transform([obs])[0]))\n",
    "\n",
    "def action_discretizer(action) -> tuple[int,...]:\n",
    "    est = KBinsDiscretizer(n_bins=n_action_bins, encode=\"ordinal\", strategy=\"uniform\")\n",
    "    est.fit([action_low, action_high])\n",
    "    return tuple(map(int, est.transform([action])[0]))\n",
    "\n",
    "def action_undiscretizer(action):\n",
    "    est = KBinsDiscretizer(n_bins=n_action_bins, encode=\"ordinal\", strategy=\"uniform\")\n",
    "    est.fit([action_low, action_high])\n",
    "    return est.inverse_transform([action])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape (11, 11, 11, 11, 3)\n"
     ]
    }
   ],
   "source": [
    "Q_shape = tuple([n_obs_bins for n in range(obs_low.shape[0])] + [n_action_bins for n in range(action_low.shape[0])])\n",
    "Q_table = np.random.uniform(low=-2, high=0, size=Q_shape)\n",
    "print(\"Shape\", Q_table.shape)\n",
    "\n",
    "save = False\n",
    "\n",
    "if save:\n",
    "    with open('q_table.npy', 'wb') as f:\n",
    "        np.save(f, Q_table)\n",
    "    print(\"Successfully saved Q_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state):\n",
    "    return (np.argmax(Q_table[state]),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_Q_value(reward: float, new_state, discount_factor=1.0) -> float:\n",
    "    future_optimal_value = np.max(Q_table[new_state])\n",
    "    learned_value = reward + discount_factor * future_optimal_value\n",
    "    return learned_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read Q_table\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5352618c6d6541f2817fb933d9b414f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=10000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. KBinsDiscretizer expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m obs, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset() \n\u001b[0;32m     36\u001b[0m done \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m state \u001b[39m=\u001b[39m obs_discretizer(refine_state(obs))\n\u001b[0;32m     39\u001b[0m ep_reward \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     41\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n",
      "Cell \u001b[1;32mIn[13], line 8\u001b[0m, in \u001b[0;36mobs_discretizer\u001b[1;34m(obs)\u001b[0m\n\u001b[0;32m      6\u001b[0m est \u001b[39m=\u001b[39m KBinsDiscretizer(n_bins\u001b[39m=\u001b[39mn_obs_bins, encode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mordinal\u001b[39m\u001b[39m\"\u001b[39m, strategy\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39muniform\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m est\u001b[39m.\u001b[39mfit([obs_low, obs_high])\n\u001b[1;32m----> 8\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mint\u001b[39m, est\u001b[39m.\u001b[39;49mtransform([obs])[\u001b[39m0\u001b[39m]))\n",
      "File \u001b[1;32mc:\\Users\\sacka\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:142\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 142\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    144\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    145\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    146\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    147\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    148\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\sacka\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:345\u001b[0m, in \u001b[0;36mKBinsDiscretizer.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[39m# check input and attribute dtypes\u001b[39;00m\n\u001b[0;32m    344\u001b[0m dtype \u001b[39m=\u001b[39m (np\u001b[39m.\u001b[39mfloat64, np\u001b[39m.\u001b[39mfloat32) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype\n\u001b[1;32m--> 345\u001b[0m Xt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, copy\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, dtype\u001b[39m=\u001b[39;49mdtype, reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    347\u001b[0m bin_edges \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbin_edges_\n\u001b[0;32m    348\u001b[0m \u001b[39mfor\u001b[39;00m jj \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(Xt\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]):\n",
      "File \u001b[1;32mc:\\Users\\sacka\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:546\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    545\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 546\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[0;32m    547\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[0;32m    548\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32mc:\\Users\\sacka\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:915\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    910\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    911\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdtype=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnumeric\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    912\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    913\u001b[0m     )\n\u001b[0;32m    914\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nd \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[1;32m--> 915\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    916\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    917\u001b[0m         \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    918\u001b[0m     )\n\u001b[0;32m    920\u001b[0m \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[0;32m    921\u001b[0m     _assert_all_finite(\n\u001b[0;32m    922\u001b[0m         array,\n\u001b[0;32m    923\u001b[0m         input_name\u001b[39m=\u001b[39minput_name,\n\u001b[0;32m    924\u001b[0m         estimator_name\u001b[39m=\u001b[39mestimator_name,\n\u001b[0;32m    925\u001b[0m         allow_nan\u001b[39m=\u001b[39mforce_all_finite \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mallow-nan\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    926\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with dim 3. KBinsDiscretizer expected <= 2."
     ]
    }
   ],
   "source": [
    "read = True\n",
    "save = True\n",
    "\n",
    "if read:\n",
    "    with open('q_table.npy', 'rb') as f:\n",
    "        Q_table = np.load(f)\n",
    "    print(\"Successfully read Q_table\")\n",
    "\n",
    "\n",
    "alpha = learning_rate = 0.1\n",
    "gamma = discount = 0.95\n",
    "epsilon = exploration_rate = 1\n",
    "\n",
    "n_episodes = 10000\n",
    "show_every = 100\n",
    "save_every = 100\n",
    "\n",
    "start_epsilon_decay = 1\n",
    "end_epsilon_decay = 4000\n",
    "epsilon_decay_value = epsilon/(end_epsilon_decay-start_epsilon_decay)\n",
    "\n",
    "progress = IntProgress(min=0, max=n_episodes) # instantiate the bar\n",
    "display(progress) # display the bar\n",
    "\n",
    "ep_rewards = []\n",
    "aggr_ep_rewards = {\n",
    "    \"ep\": [],\n",
    "    \"avg\": [],\n",
    "    \"min\": [],\n",
    "    \"max\": []\n",
    "}\n",
    "\n",
    "for e in range(n_episodes):\n",
    "    progress.value += 1\n",
    "    obs, _ = env.reset() \n",
    "    done = False\n",
    "    state = obs_discretizer(refine_state(obs))\n",
    "\n",
    "    ep_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = policy(state)\n",
    "        exploring = False\n",
    "\n",
    "        if np.random.random() < epsilon:\n",
    "            action = (np.random.randint(0, n_action_bins),)\n",
    "            exploring = True\n",
    "        \n",
    "        action_continuous = action_undiscretizer(action)\n",
    "        obs, reward, done, msg, _ = env.step(action_continuous)\n",
    "        ep_reward += reward\n",
    "\n",
    "        new_state = obs_discretizer(refine_state(obs))\n",
    "\n",
    "        if not done:\n",
    "            Q_future_max = np.max(Q_table[new_state + action])\n",
    "            Q_current = Q_table[new_state + action]\n",
    "\n",
    "            Q_new = (1-alpha) * Q_current + alpha * (reward + gamma * Q_future_max)\n",
    "            Q_table[state + action] = Q_new\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "        if not e % show_every:\n",
    "            env.render([\n",
    "                \"\",\n",
    "                f\"Episode: {e}\",\n",
    "                f\"Reward: {reward}\",\n",
    "                f\"Exploration rate: {round(epsilon, 3)}\",\n",
    "                f\"Exploring: {exploring}\"\n",
    "            ])\n",
    "            time.sleep(dt)\n",
    "\n",
    "        if msg[\"won\"]:\n",
    "            print(f\"Won! on episode {e}\")\n",
    "\n",
    "    if end_epsilon_decay >= e >= start_epsilon_decay:\n",
    "        epsilon -= epsilon_decay_value\n",
    "\n",
    "    ep_rewards.append(ep_reward)\n",
    "\n",
    "    if not e % save_every:\n",
    "        average_reward = sum(ep_rewards[-save_every:])/len(ep_rewards[-save_every:])\n",
    "        min_reward = min(ep_rewards[-save_every:])\n",
    "        max_reward = max(ep_rewards[-save_every:])\n",
    "        aggr_ep_rewards[\"ep\"].append(e)\n",
    "        aggr_ep_rewards[\"avg\"].append(average_reward)\n",
    "        aggr_ep_rewards[\"min\"].append(min_reward)\n",
    "        aggr_ep_rewards[\"max\"].append(max_reward)\n",
    "\n",
    "        print(f\"Episode: {e}, avg: {average_reward}, min: {min_reward} max: {max_reward}\")\n",
    "        \n",
    "        if save:\n",
    "            with open('q_table.npy', 'wb') as f:\n",
    "                np.save(f, Q_table)\n",
    "            print(\"Successfully saved Q_table\")\n",
    "        \n",
    "env.close()\n",
    "\n",
    "plt.plot(aggr_ep_rewards[\"ep\"], aggr_ep_rewards[\"avg\"], label=\"avg\")\n",
    "plt.plot(aggr_ep_rewards[\"ep\"], aggr_ep_rewards[\"min\"], label=\"min\")\n",
    "plt.plot(aggr_ep_rewards[\"ep\"], aggr_ep_rewards[\"max\"], label=\"max\")\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "\n",
    "if save:\n",
    "    with open('q_table.npy', 'wb') as f:\n",
    "        np.save(f, Q_table)\n",
    "    print(\"Successfully saved Q_table\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
