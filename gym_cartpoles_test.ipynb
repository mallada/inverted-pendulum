{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "from math import sin, cos, radians, log10\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import numpy as np\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "from cartpoles import Cart, DCMotor, Pole\n",
    "from colors import Colors\n",
    "from cartpolesenv import CartPolesEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\gym\\spaces\\box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cart = Cart(0.5, 0.05, 0, -0.8, 0.8, Colors.red,\n",
    "    DCMotor(-12, 12, 0.05, 0.5, 0.05, 0.01, 0.05, Colors.black),\n",
    "    Pole(0.2, radians(10), 0.2, 0.005, Colors.green, None \n",
    "    )\n",
    ")\n",
    "\n",
    "dt = 0.01\n",
    "g = 9.81\n",
    "env = CartPolesEnv(cart, dt, g)\n",
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs_bins = 10\n",
    "n_action_bins = 2\n",
    "\n",
    "obs_est = KBinsDiscretizer(n_bins=n_obs_bins, encode=\"ordinal\", strategy=\"uniform\")\n",
    "action_est = KBinsDiscretizer(n_bins=n_action_bins, encode=\"ordinal\", strategy=\"uniform\")\n",
    "\n",
    "def discretizer(data, est: KBinsDiscretizer, low: list[float], high: list[float]) -> tuple[int,...]:\n",
    "    est.fit([low, high])\n",
    "    return tuple(map(int, est.transform([data])[0]))\n",
    "\n",
    "def obs_discretizer(obs):\n",
    "    return discretizer(obs, obs_est, env.observation_space.low, env.observation_space.high)\n",
    "\n",
    "def action_discretizer(action):\n",
    "    return discretizer(action, action_est, env.action_space.low, env.action_space.high)\n",
    "\n",
    "def undiscretizer(data: int, est: KBinsDiscretizer, low: list[float], high: list[float]):\n",
    "    est.fit([low, high])\n",
    "    return est.inverse_transform([data])[0]\n",
    "\n",
    "def obs_undiscretizer(obs):\n",
    "    return undiscretizer(obs, obs_est, env.observation_space.low, env.observation_space.high)\n",
    "\n",
    "def action_undiscretizer(action):\n",
    "    return undiscretizer(action, action_est, env.action_space.low, env.action_space.high)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10, 10, 10, 10, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_table = np.zeros(tuple(np.repeat(np.array([n_obs_bins]), env.observation_space.shape[0])) + tuple(np.repeat(np.array([n_action_bins]), env.action_space.shape[0])), dtype=int)\n",
    "print(\"Sum\", Q_table.sum())\n",
    "Q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state: tuple):\n",
    "    return (np.argmax(Q_table[state]),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_Q_value(reward: float, new_state: tuple, discount_factor=1.0) -> float:\n",
    "    future_optimal_value = np.max(Q_table[new_state])\n",
    "    learned_value = reward + discount_factor * future_optimal_value\n",
    "    return learned_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate(n: int, min_rate=0.1) -> float:\n",
    "    return max(min_rate, min(1.0, 1.0 - log10((n+1)/100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exploration_rate(n: int, min_rate=0.05) -> float:\n",
    "    return max(min_rate, min(1, 1.0-log10((n+1)/100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d09ca0812a8745528b03ad3cdfec352f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=10000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_episodes = 10000\n",
    "\n",
    "f = IntProgress(min=0, max=n_episodes) # instantiate the bar\n",
    "display(f) # display the bar\n",
    "\n",
    "env_e = env\n",
    "for e in range(n_episodes):\n",
    "    f.value += 1\n",
    "\n",
    "    obs, info = env_e.reset()\n",
    "    current_state, done = obs_discretizer(obs), False\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    while not done:\n",
    "        action = policy(current_state)\n",
    "        # print(\"policy action!\", action)\n",
    "\n",
    "        er = exploration_rate(e)\n",
    "        exploring = np.random.random() < er\n",
    "        if exploring:\n",
    "            action = action_discretizer(env_e.action_space.sample())\n",
    "            # print(\"random!\")\n",
    "\n",
    "        # print(\"action\", action)\n",
    "        # print(\"undiscretized\", action_undiscretizer(action))\n",
    "\n",
    "        obs, reward, done, info, _ = env_e.step(*action_undiscretizer(action))\n",
    "\n",
    "        if i == 0 and done:\n",
    "            print(obs)\n",
    "            print(\"x\", env_e.cart.position())\n",
    "            print(\"max_x\", env_e.cart.max_x)\n",
    "            print(\"min_x\", env_e.cart.min_x)\n",
    "\n",
    "            print(\"y\", env_e.cart.height())\n",
    "\n",
    "        new_state = obs_discretizer(obs)\n",
    "\n",
    "        lr = learning_rate(e)\n",
    "        learnt_value = new_Q_value(reward, new_state, 1)\n",
    "        old_value = Q_table[current_state][action]\n",
    "        Q_table[current_state][action] = (1-lr)*old_value + lr*learnt_value\n",
    "\n",
    "        current_state = new_state\n",
    "        \n",
    "        if e % 100 == 0:\n",
    "            env.render([\n",
    "                \"\",\n",
    "                f\"Learning rate: {round(lr,2)}\",\n",
    "                f\"Exploration rate: {round(er,2)}\",\n",
    "                \"Exploring!\" if exploring else \"Not exploring\",\n",
    "                f\"Reward: {reward}\"\n",
    "            ])\n",
    "            time.sleep(dt)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20016"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_table.sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
