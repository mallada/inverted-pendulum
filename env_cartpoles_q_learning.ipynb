{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "from math import sin, cos, radians, log10\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import numpy as np\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "from lib.cartpoles import CartPoleSystem, CartPolesEnv\n",
    "from lib.colors import Colors\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\gym\\spaces\\box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = 0.01\n",
    "g = 9.81\n",
    "\n",
    "system = CartPoleSystem(\n",
    "    (0.0, 0.5, 0.05, -0.8, 0.8, Colors.red),\n",
    "    (0.05, 0.05, 0.01, 0.5, 0.05, -24.0, 24.0, Colors.black),\n",
    "    [\n",
    "        (radians(10), 0.2, 0.2, 0.005, Colors.green),\n",
    "    ],\n",
    "    g,\n",
    "    \"rk4\"\n",
    ")\n",
    "\n",
    "env = CartPolesEnv(system, dt, g)\n",
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low [-0.80000001 -5.          0.         -5.        ]\n",
      "High [0.80000001 5.         6.28318531 5.        ]\n"
     ]
    }
   ],
   "source": [
    "n_obs_bins = 11\n",
    "n_action_bins = 3\n",
    "max_velocity = 5\n",
    "\n",
    "obs_low = np.array(env.observation_space.low)\n",
    "obs_high = np.array(env.observation_space.high)\n",
    "\n",
    "obs_low = np.hstack([np.array([obs_low[0], -max_velocity]), np.tile([0, -max_velocity], system.num_poles)])\n",
    "obs_high = np.hstack([np.array([obs_high[0], max_velocity]), np.tile([radians(360), max_velocity], system.num_poles)])\n",
    "\n",
    "action_low = np.array(env.action_space.low)\n",
    "action_high = np.array(env.action_space.high)\n",
    "\n",
    "print(\"Low\", obs_low)\n",
    "print(\"High\", obs_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_state(state):\n",
    "    refined = state\n",
    "    return refined\n",
    "\n",
    "def obs_discretizer(obs) -> tuple[int,...]:\n",
    "    est = KBinsDiscretizer(n_bins=n_obs_bins, encode=\"ordinal\", strategy=\"uniform\")\n",
    "    est.fit([obs_low, obs_high])\n",
    "    return tuple(map(int, est.transform([obs])[0]))\n",
    "\n",
    "def action_discretizer(action) -> tuple[int,...]:\n",
    "    est = KBinsDiscretizer(n_bins=n_action_bins, encode=\"ordinal\", strategy=\"uniform\")\n",
    "    est.fit([action_low, action_high])\n",
    "    return tuple(map(int, est.transform([action])[0]))\n",
    "\n",
    "def action_undiscretizer(action):\n",
    "    est = KBinsDiscretizer(n_bins=n_action_bins, encode=\"ordinal\", strategy=\"uniform\")\n",
    "    est.fit([action_low, action_high])\n",
    "    return est.inverse_transform([action])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape (11, 11, 11, 11, 3)\n"
     ]
    }
   ],
   "source": [
    "Q_shape = tuple([n_obs_bins for n in range(obs_low.shape[0])] + [n_action_bins for n in range(action_low.shape[0])])\n",
    "Q_table = np.random.uniform(low=-2, high=0, size=Q_shape)\n",
    "print(\"Shape\", Q_table.shape)\n",
    "\n",
    "save = False\n",
    "\n",
    "if save:\n",
    "    with open('q_table.npy', 'wb') as f:\n",
    "        np.save(f, Q_table)\n",
    "    print(\"Successfully saved Q_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state):\n",
    "    return (np.argmax(Q_table[state]),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_Q_value(reward: float, new_state, discount_factor=1.0) -> float:\n",
    "    future_optimal_value = np.max(Q_table[new_state])\n",
    "    learned_value = reward + discount_factor * future_optimal_value\n",
    "    return learned_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read Q_table\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b888313588914d03b8667aafcc734e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=10000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, avg: -630.0, min: -630 max: -630\n",
      "Successfully saved Q_table\n"
     ]
    }
   ],
   "source": [
    "read = True\n",
    "save = True\n",
    "\n",
    "if read:\n",
    "    with open('q_table.npy', 'rb') as f:\n",
    "        Q_table = np.load(f)\n",
    "    print(\"Successfully read Q_table\")\n",
    "\n",
    "\n",
    "alpha = learning_rate = 0.1\n",
    "gamma = discount = 0.95\n",
    "epsilon = exploration_rate = 1\n",
    "\n",
    "n_episodes = 10000\n",
    "show_every = 100\n",
    "save_every = 100\n",
    "\n",
    "start_epsilon_decay = 1\n",
    "end_epsilon_decay = 4000\n",
    "epsilon_decay_value = epsilon/(end_epsilon_decay-start_epsilon_decay)\n",
    "\n",
    "progress = IntProgress(min=0, max=n_episodes) # instantiate the bar\n",
    "display(progress) # display the bar\n",
    "\n",
    "ep_rewards = []\n",
    "aggr_ep_rewards = {\n",
    "    \"ep\": [],\n",
    "    \"avg\": [],\n",
    "    \"min\": [],\n",
    "    \"max\": []\n",
    "}\n",
    "\n",
    "for e in range(n_episodes):\n",
    "    progress.value += 1\n",
    "    obs, _ = env.reset() \n",
    "    done = False\n",
    "    state = obs_discretizer(refine_state(obs))\n",
    "\n",
    "    ep_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = policy(state)\n",
    "        exploring = False\n",
    "\n",
    "        if np.random.random() < epsilon:\n",
    "            action = (np.random.randint(0, n_action_bins),)\n",
    "            exploring = True\n",
    "        \n",
    "        action_continuous = action_undiscretizer(action)\n",
    "        obs, reward, done, msg, _ = env.step(action_continuous)\n",
    "        ep_reward += reward\n",
    "\n",
    "        new_state = obs_discretizer(refine_state(obs))\n",
    "\n",
    "        if not done:\n",
    "            Q_future_max = np.max(Q_table[new_state + action])\n",
    "            Q_current = Q_table[new_state + action]\n",
    "\n",
    "            Q_new = (1-alpha) * Q_current + alpha * (reward + gamma * Q_future_max)\n",
    "            Q_table[state + action] = Q_new\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "        if not e % show_every:\n",
    "            env.render([\n",
    "                \"\",\n",
    "                f\"Episode: {e}\",\n",
    "                f\"Reward: {reward}\",\n",
    "                f\"Exploration rate: {round(epsilon, 3)}\",\n",
    "                f\"Exploring: {exploring}\"\n",
    "            ])\n",
    "            time.sleep(dt)\n",
    "\n",
    "        if msg[\"won\"]:\n",
    "            print(f\"Won! on episode {e}\")\n",
    "\n",
    "    if end_epsilon_decay >= e >= start_epsilon_decay:\n",
    "        epsilon -= epsilon_decay_value\n",
    "\n",
    "    ep_rewards.append(ep_reward)\n",
    "\n",
    "    if not e % save_every:\n",
    "        average_reward = sum(ep_rewards[-save_every:])/len(ep_rewards[-save_every:])\n",
    "        min_reward = min(ep_rewards[-save_every:])\n",
    "        max_reward = max(ep_rewards[-save_every:])\n",
    "        aggr_ep_rewards[\"ep\"].append(e)\n",
    "        aggr_ep_rewards[\"avg\"].append(average_reward)\n",
    "        aggr_ep_rewards[\"min\"].append(min_reward)\n",
    "        aggr_ep_rewards[\"max\"].append(max_reward)\n",
    "\n",
    "        print(f\"Episode: {e}, avg: {average_reward}, min: {min_reward} max: {max_reward}\")\n",
    "        \n",
    "        if save:\n",
    "            with open('q_table.npy', 'wb') as f:\n",
    "                np.save(f, Q_table)\n",
    "            print(\"Successfully saved Q_table\")\n",
    "        \n",
    "env.close()\n",
    "\n",
    "plt.plot(aggr_ep_rewards[\"ep\"], aggr_ep_rewards[\"avg\"], label=\"avg\")\n",
    "plt.plot(aggr_ep_rewards[\"ep\"], aggr_ep_rewards[\"min\"], label=\"min\")\n",
    "plt.plot(aggr_ep_rewards[\"ep\"], aggr_ep_rewards[\"max\"], label=\"max\")\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "\n",
    "if save:\n",
    "    with open('q_table.npy', 'wb') as f:\n",
    "        np.save(f, Q_table)\n",
    "    print(\"Successfully saved Q_table\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
