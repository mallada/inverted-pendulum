{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "from math import sin, cos, radians, log10\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import numpy as np\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "from lib.cartpolesystem import CartPoleSystem\n",
    "from lib.colors import Colors\n",
    "from lib.cartpolesenv import CartPolesEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\gym\\spaces\\box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = 0.025\n",
    "g = 9.81\n",
    "\n",
    "system = CartPoleSystem(\n",
    "    (0.0, 0.5, 0.05, -0.8, 0.8, Colors.red),\n",
    "    (0.05, 0.05, 0.01, 0.5, 0.05, -24.0, 24.0, Colors.black),\n",
    "    [\n",
    "        (radians(10), 0.2, 0.2, 0.005, Colors.green),\n",
    "    ],\n",
    "    g,\n",
    "    \"rk4\"\n",
    ")\n",
    "\n",
    "env = CartPolesEnv(system, dt, g)\n",
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs_bins = 10\n",
    "n_action_bins = 2\n",
    "\n",
    "obs_est = KBinsDiscretizer(n_bins=n_obs_bins, encode=\"ordinal\", strategy=\"uniform\")\n",
    "action_est = KBinsDiscretizer(n_bins=n_action_bins, encode=\"ordinal\", strategy=\"uniform\")\n",
    "\n",
    "obs_low = env.observation_space.low\n",
    "obs_high = env.observation_space.high\n",
    "\n",
    "def discretizer(data, est: KBinsDiscretizer, low: list[float], high: list[float]) -> tuple[int,...]:\n",
    "    est.fit([low, high])\n",
    "    return tuple(map(int, est.transform([data])[0]))\n",
    "\n",
    "def obs_discretizer(obs):\n",
    "    return discretizer(obs, obs_est, obs_low, obs_high)\n",
    "\n",
    "def action_discretizer(action):\n",
    "    return discretizer(action, action_est, env.action_space.low, env.action_space.high)\n",
    "\n",
    "def undiscretizer(data: int, est: KBinsDiscretizer, low: list[float], high: list[float]):\n",
    "    est.fit([low, high])\n",
    "    return est.inverse_transform([data])[0]\n",
    "\n",
    "def obs_undiscretizer(obs):\n",
    "    return undiscretizer(obs, obs_est, obs_low, obs_high)\n",
    "\n",
    "def action_undiscretizer(action):\n",
    "    return undiscretizer(action, action_est, env.action_space.low, env.action_space.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10, 10, 10, 10, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_table = np.zeros(tuple(np.repeat(np.array([n_obs_bins]), obs_low.shape[0])) + tuple(np.repeat(np.array([n_action_bins]), env.action_space.shape[0])), dtype=int)\n",
    "print(\"Sum\", Q_table.sum())\n",
    "\n",
    "save = False\n",
    "\n",
    "if save:\n",
    "    with open('q_table.npy', 'wb') as f:\n",
    "        np.save(f, Q_table)\n",
    "Q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state: tuple):\n",
    "    return (np.argmax(Q_table[state]),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_Q_value(reward: float, new_state: tuple, discount_factor=1.0) -> float:\n",
    "    future_optimal_value = np.max(Q_table[new_state])\n",
    "    learned_value = reward + discount_factor * future_optimal_value\n",
    "    return learned_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate(n: int, min_rate=0.1) -> float:\n",
    "    return max(min_rate, min(1.0, 1.0 - log10((n+1)/100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exploration_rate(n: int, min_rate=0.05) -> float:\n",
    "    return max(min_rate, min(1, 1.0-log10((n+1)/100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before sum 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13abe39692e94df09ebadb6b1c2c61cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=10000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('q_table.npy', 'rb') as f:\n",
    "    Q_table = np.load(f)\n",
    "print(\"Before sum\", Q_table.sum())\n",
    "\n",
    "n_episodes = 10000\n",
    "\n",
    "f = IntProgress(min=0, max=n_episodes) # instantiate the bar\n",
    "display(f) # display the bar\n",
    "\n",
    "env_e = env\n",
    "for e in range(2000,n_episodes):\n",
    "    f.value += 1\n",
    "\n",
    "    obs, info = env_e.reset()\n",
    "    current_state, done = obs_discretizer(obs), False\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    while not done:\n",
    "        action = policy(current_state)\n",
    "        # print(\"policy action!\", action)\n",
    "\n",
    "        er = exploration_rate(e)\n",
    "        exploring = np.random.random() < er\n",
    "        if exploring:\n",
    "            action = action_discretizer(env_e.action_space.sample())\n",
    "            # print(\"random!\")\n",
    "\n",
    "        # print(\"action\", action)\n",
    "        # print(\"undiscretized\", action_undiscretizer(action))\n",
    "\n",
    "        obs, reward, done, info, _ = env_e.step(*action_undiscretizer(action))\n",
    "\n",
    "        if i == 0 and done:\n",
    "            print(obs)\n",
    "            print(\"x\", obs[0])\n",
    "            print(\"max_x\", env_e.system.max_x)\n",
    "            print(\"min_x\", env_e.system.min_x)\n",
    "\n",
    "            print(\"y\", env_e.system.end_height())\n",
    "\n",
    "        new_state = obs_discretizer(obs)\n",
    "\n",
    "        lr = learning_rate(e)\n",
    "        learnt_value = new_Q_value(reward, new_state, 1)\n",
    "        old_value = Q_table[current_state][action]\n",
    "        Q_table[current_state][action] = (1-lr)*old_value + lr*learnt_value\n",
    "\n",
    "        current_state = new_state\n",
    "        \n",
    "        if e % 100 == 0:\n",
    "            env.render([\n",
    "                \"\",\n",
    "                f\"Learning rate: {round(lr,2)}\",\n",
    "                f\"Exploration rate: {round(er,2)}\",\n",
    "                \"Exploring!\" if exploring else \"Not exploring\",\n",
    "                f\"Reward: {round(reward,2)}\"\n",
    "            ])\n",
    "            time.sleep(dt)\n",
    "        i += 1\n",
    "\n",
    "with open('q_table.npy', 'wb') as f:\n",
    "    np.save(f, Q_table)\n",
    "print(\"After sum:\", Q_table.sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
